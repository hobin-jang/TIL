{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# document transformers\n",
    "# 데이터를 조금씩 분리시키는 모듈 \n",
    "# 분리된 데이터를 바탕으로 embedding 진행 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ref : \n",
    "\n",
    "https://colab.research.google.com/drive/1EPdHkk8qK-dxMqjlVvGj07ey3ZkZLZ5j?usp=sharing%EF%BB%BF#scrollTo=FSTi00anFGqf\n",
    "\n",
    "https://python.langchain.com/docs/modules/data_connection/document_transformers/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 예제 txt : example.txt \n",
    "with open(\"./example.txt\", encoding=\"utf-8\") as f :\n",
    "    llm_example_text = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'대형 언어 모델(Large language model, LLM) 또는 거대 언어 모델은 수많은 파라미터(보통 수십억 웨이트 이상)를 보유한 인공 신경망으로 구성되는 언어 모델이다. 자기 지도 학습이나 반자기지도학습을 사용하여 레이블링되지 않은 상당한 양의 텍스트로 훈련된다.[1] LLM은 2018년 즈음에 모습을 드러냈으며 다양한 작업을 위해 수행된다. 이전의 특정 작업의 특수한 지도 학습 모델의 훈련 패러다임에서 벗어나 자연어 처리 연구로 초점이 옮겨졌다.\\n\\n대규모 언어 모델(LLM) 은 AI 챗봇 기술을 가능하게 하는 요소이며 많은 화제를 불러일으키고 있는 주제 중 하나다. 대규모 언어 모델(LLM)의 작동 방식은 크게 3가지로 나뉘고 있다. 토큰화, 트랜스포머 모델, 프롬프트 등. 토큰화는 자연어 처리의 일부로 일반 인간 언어를 저수준 기계 시스템(LLMS)가 이해할 수 있는 시퀀스로 변환하는 작업을 말하며 여기에는 섹션에 숫자 값을 할당하고 빠른 분석을 위해 인코딩하는 작업이 수반된다. 이는 음성학의 AI 버전과 같으며 토큰화의 목적은 인공지능이 문장의 구조를 예측하기 위한 학습 가이드 또는 공식과 같은 컨텍스트 백터를 생성하는 것이 목적. 언어를 더 많이 연구하고 문장이 어떻게 구성되는지 이해할수록 특정 유형의 문장에서 다음 언어에 대한 예측이 더 정확 해진다. 이로 인해 온라인에서 사람들이 사용하는 다양한 커뮤니케이션 스타일을 재현하는 모델을 개발할 수 있다.\\n\\n트랜스포머 모델은 순차적 데이터를 검사하여 어떤 단어가 서로 뒤따를 가능성이 높은지 관련 패턴을 식별하는 신경망의 일종으로 각각 다른 분석을 수행하여 어떤 단어가 호환되는지 결정하는 계층으로 구성된다. 이러한 모델은 언어를 학습하지 않고 알고리즘에 의존하여 사람이 쓴 단어를 이해하고 예를들어, 힙스터 커피 블로그를 제공함으로써 커피에 대한 표준 글을 작성하도록 학습 시킨다. 이 트랜스포머 모델이 대규모 언어 모델 LLM 언어 생성의 기초.\\n\\n프롬프트는 개발자가 정보를 분석하고 토큰화하기 위해 대규모 언어 모델 LLM에 제공하는 정보로 프롬프트는 기본적으로 다양한 사용 사례에서 LLM에 도움이 되는 학습 데이터 입니다. 더 정확한 프롬프트를 받을수록 LLM은 다음 단어를 더 잘 예측하고 정확한 문장을 구성할 수 있습니다. 따라서 딥러닝 AI의 적절한 학습을 위해서는 적절한 프롬프트를 선택하는 것이 중요하다.'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm_example_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RecursiveCharacterTextSplitter\n",
    "# 많이 사용하는 text splitter\n",
    "# 구분 기호 : \"\\n\\n\", \"\\n\", \" \" 등\n",
    "# length_function : chunk 길이 계산 방법 (default : 문자 길이로 지정) (토큰 개수로 지정하는 경우도 많음)\n",
    "# chunk_size : chunk 최대 길이 \n",
    "# chunk_overlap : chunk 간 중복 크기 (자연스러운 연결 위해 중복 지정하는 것이 좋음)\n",
    "# add_start_index : chunk 시작 위치를 metadata에 포함할지 여부를 결정\n",
    "\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size = 100,\n",
    "    chunk_overlap = 20,\n",
    "    length_function = len,\n",
    "    add_start_index = True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = text_splitter.create_documents([llm_example_text])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16\n",
      "page_content='대형 언어 모델(Large language model, LLM) 또는 거대 언어 모델은 수많은 파라미터(보통 수십억 웨이트 이상)를 보유한 인공 신경망으로 구성되는 언어 모델이다.' metadata={'start_index': 0}\n",
      "page_content='신경망으로 구성되는 언어 모델이다. 자기 지도 학습이나 반자기지도학습을 사용하여 레이블링되지 않은 상당한 양의 텍스트로 훈련된다.[1] LLM은 2018년 즈음에 모습을' metadata={'start_index': 81}\n",
      "100\n",
      "94\n"
     ]
    }
   ],
   "source": [
    "print(len(texts))\n",
    "print(texts[0])\n",
    "print(texts[1])\n",
    "\n",
    "print(len(texts[0].page_content))\n",
    "print(len(texts[1].page_content))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# HTMLHeaderTextSplitter\n",
    "# 지정한 html 태그를 기준으로 html 내용을 split\n",
    "\n",
    "from langchain.text_splitter import HTMLHeaderTextSplitter\n",
    "\n",
    "html_string = \"\"\"\n",
    "<!DOCTYPE html>\n",
    "<html>\n",
    "<body>\n",
    "    <div>\n",
    "        <h1>Foo</h1>\n",
    "        <p>Some intro text about Foo.</p>\n",
    "        <div>\n",
    "            <h2>Bar main section</h2>\n",
    "            <p>Some intro text about Bar.</p>\n",
    "            <h3>Bar subsection 1</h3>\n",
    "            <p>Some text about the first subtopic of Bar.</p>\n",
    "            <h3>Bar subsection 2</h3>\n",
    "            <p>Some text about the second subtopic of Bar.</p>\n",
    "        </div>\n",
    "        <div>\n",
    "            <h2>Baz</h2>\n",
    "            <p>Some text about Baz</p>\n",
    "        </div>\n",
    "        <br>\n",
    "        <p>Some concluding text about Foo</p>\n",
    "    </div>\n",
    "</body>\n",
    "</html>\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "headers_to_split_on = [\n",
    "    (\"h1\", \"header 1\"),\n",
    "    (\"h2\", \"header 2\"),\n",
    "]\n",
    "\n",
    "html_splitter = HTMLHeaderTextSplitter(headers_to_split_on=headers_to_split_on)\n",
    "html_header_splits = html_splitter.split_text(html_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='Foo'),\n",
       " Document(page_content='Some intro text about Foo.  \\nBar main section Bar subsection 1 Bar subsection 2', metadata={'header 1': 'Foo'}),\n",
       " Document(page_content='Some intro text about Bar.  \\nSome text about the first subtopic of Bar.  \\nSome text about the second subtopic of Bar.', metadata={'header 1': 'Foo', 'header 2': 'Bar main section'}),\n",
       " Document(page_content='Baz', metadata={'header 1': 'Foo'}),\n",
       " Document(page_content='Some text about Baz', metadata={'header 1': 'Foo', 'header 2': 'Baz'}),\n",
       " Document(page_content='Some concluding text about Foo', metadata={'header 1': 'Foo'})]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "html_header_splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 웹에서 직접 불러오기 \n",
    "url = \"https://ko.wikipedia.org/wiki/%EB%8C%80%ED%98%95_%EC%96%B8%EC%96%B4_%EB%AA%A8%EB%8D%B8\"\n",
    "\n",
    "headers_to_split_on = [\n",
    "    (\"h1\", \"header 1\"),\n",
    "    (\"h2\", \"header 2\"),\n",
    "    (\"h3\", \"header 3\"),\n",
    "    (\"h4\", \"header 4\"),\n",
    "]\n",
    "\n",
    "html_splitter = HTMLHeaderTextSplitter(headers_to_split_on=headers_to_split_on)\n",
    "html_header_splits = html_splitter.split_text_from_url(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='주 메뉴  \\n사이드바로 이동 숨기기  \\n주 메뉴  \\n둘러보기  \\n대문최근 바뀜요즘 화제임의의 문서로기부  \\n사용자 모임  \\n사랑방사용자 모임관리 요청  \\n편집 안내  \\n소개도움말정책과 지침질문방  \\n검색  \\n검색  \\n계정 만들기 로그인  \\n개인 도구  \\n계정 만들기 로그인  \\n로그아웃한 편집자를 위한 문서 더 알아보기  \\n기여토론  \\n목차 사이드바로 이동 숨기기  \\n처음 위치  \\n1대형 언어 모델 목록  \\n2각주  \\n대형 언어 모델'),\n",
       " Document(page_content=\"목차 토글  \\n36개 언어  \\nالعربيةAzərbaycancaBoarischBosanskiCatalàČeštinaDeutschΕλληνικάEnglishEspañolEuskaraفارسیFrançaisGaeilgeGalegoעבריתहिन्दीBahasa IndonesiaItaliano日本語МакедонскиNederlandsPortuguêsRuna SimiРусскийSlovenščinaСрпски / srpskiTagalogTürkçeئۇيغۇرچە / UyghurcheУкраїнськаTiếng Việt中文閩南語 / Bân-lâm-gú粵語IsiZulu  \\n링크 편집  \\n문서토론  \\n한국어  \\n읽기편집역사 보기  \\n도구  \\n사이드바로 이동 숨기기  \\n도구  \\n동작  \\n읽기편집역사 보기  \\n일반  \\n여기를 가리키는 문서가리키는 글의 최근 바뀜파일 올리기특수 문서 목록고유 링크문서 정보이 문서 인용하기축약된 URL 얻기QR 코드 다운로드위키데이터 항목  \\n인쇄/내보내기  \\n책 만들기PDF로 다운로드인쇄용 판  \\n위키백과, 우리 모두의 백과사전.  \\n대형 언어 모델 목록[편집] 대형 언어 모델 목록 이름 출시일[a] 개발 파라미터 수[b] 코퍼스 크기 라이선스[c] BERT 2018년 구글 340 million[2] 3.3 billion words[2] Apache 2.0[3] XLNet 2019년 Google ~340 million[4] 33 billion words GPT-2 2019년 OpenAI 1.5 billion[5] 40GB[6] (~10 billion tokens)[7] MIT[8] GPT-3 2020년 OpenAI 175 billion[9] 300 billion tokens[7] 공개 웹 API GPT-Neo 2021년 3월 EleutherAI 2.7 billion[10] 825 GiB[11] MIT[12] GPT-J 2021년 6월 EleutherAI 6 billion[13] 825 GiB[11] Apache 2.0 Megatron-Turing NLG 2021년 10월[14] Microsoft and Nvidia 530 billion[15] 338.6 billion tokens[15] 제한된 웹 접근 Ernie 3.0 Titan 2021년 12월 Baidu 260 billion[16] 4 Tb 사유(Proprietary) Claude[17] 2021년 12월 Anthropic 52 billion[18] 400 billion tokens[18] 클로즈드 베타 GLaM (Generalist Language Model) 2021년 12월 Google 1.2 trillion[19] 1.6 trillion tokens[19] 사유(Proprietary) Gopher 2021년 12월 DeepMind 280 billion[20] 300 billion tokens[21] 사유(Proprietary) LaMDA (Language Models for Dialog Applications) 2022년 1월 Google 137 billion[22] 1.56T words,[22] 168 billion tokens[21] 사유(Proprietary) GPT-NeoX 2022년 2월 EleutherAI 20 billion[23] 825 GiB[11] Apache 2.0 Chinchilla 2022년 3월 DeepMind 70 billion[24] 1.4 trillion tokens[24][21] 사유(Proprietary) PaLM (Pathways Language Model) 2022년 4월 Google 540 billion[25] 768 billion tokens[24] 사유(Proprietary) OPT (Open Pretrained Transformer) 2022년 5월 Meta 175 billion[26] 180 billion tokens[27] 비상업적 연구[d] YaLM 100B 2022년 6월 Yandex 100 billion[28] 1.7TB[28] Apache 2.0 Minerva 2022년 6월 Google 540 billion[29] 38.5B tokens from webpages filtered for mathematical content and from papers submitted to the arXiv preprint server[29] 사유(Proprietary) BLOOM 2022년 7월 Large collaboration led by Hugging Face 175 billion[30] 350 billion tokens (1.6TB)[31] Responsible AI Galactica 2022년 11월 Meta 120 billion 106 billion tokens[32] CC-BY-NC-4.0 AlexaTM (Teacher Models) 2022년 11월 Amazon 20 billion[33] 1.3 trillion[34] 공개 웹 API[35] LLaMA (Large Language Model Meta AI) 2023년 2월 Meta 65 billion[36] 1.4 trillion[36] 비상업적 연구[e] GPT-4 2023년 3월 OpenAI 정확한 수치 알 수 없음. 대략 1 trillion [f] 알 수 없음 공개 웹 API Cerebras-GPT 2023년 3월 Cerebras 13 billion[38] Apache 2.0 Falcon 2023년 3월 Technology Innovation Institute 40 billion[39] 1 Trillion tokens (1TB)[39] 사유(Proprietary) BloombergGPT 2023년 3월 Bloomberg L.P. 50 billion 363 billion token dataset based on Bloomberg's data sources, plus 345 billion tokens from general purpose datasets[40] 사유(Proprietary) PanGu-Σ 2023년 3월 Huawei 1.085 trillion 329 billion tokens[41] 사유(Proprietary) OpenAssistant[42] 2023년 3월 LAION 17 billion 1.5 trillion tokens Apache 2.0 PaLM 2 (Pathways Language Model 2) 2023년 5월 Google 340 billion[43] 3.6 trillion tokens[43] 사유(Proprietary) 각주[편집]  \\n대형 언어 모델(Large language model, LLM) 또는 거대 언어 모델은 수많은 파라미터(보통 수십억 웨이트 이상)를 보유한 인공 신경망으로 구성되는 언어 모델이다. 자기 지도 학습이나 반자기지도학습을 사용하여 레이블링되지 않은 상당한 양의 텍스트로 훈련된다.[1] LLM은 2018년 즈음에 모습을 드러냈으며 다양한 작업을 위해 수행된다. 이전의 특정 작업의 특수한 지도 학습 모델의 훈련 패러다임에서 벗어나 자연어 처리 연구로 초점이 옮겨졌다.  \\n대규모 언어 모델(LLM) 은 AI 챗봇 기술을 가능하게 하는 요소이며 많은 화제를 불러일으키고 있는 주제 중 하나다. 대규모 언어 모델(LLM)의 작동 방식은 크게 3가지로 나뉘고 있다. 토큰화, 트랜스포머 모델, 프롬프트 등. 토큰화는 자연어 처리의 일부로 일반 인간 언어를 저수준 기계 시스템(LLMS)가 이해할 수 있는 시퀀스로 변환하는 작업을 말하며 여기에는 섹션에 숫자 값을 할당하고 빠른 분석을 위해 인코딩하는 작업이 수반된다. 이는 음성학의 AI 버전과 같으며 토큰화의 목적은 인공지능이 문장의 구조를 예측하기 위한 학습 가이드 또는 공식과 같은 컨텍스트 백터를 생성하는 것이 목적. 언어를 더 많이 연구하고 문장이 어떻게 구성되는지 이해할수록 특정 유형의 문장에서 다음 언어에 대한 예측이 더 정확 해진다. 이로 인해 온라인에서 사람들이 사용하는 다양한 커뮤니케이션 스타일을 재현하는 모델을 개발할 수 있다.  \\n트랜스포머 모델은 순차적 데이터를 검사하여 어떤 단어가 서로 뒤따를 가능성이 높은지 관련 패턴을 식별하는 신경망의 일종으로 각각 다른 분석을 수행하여 어떤 단어가 호환되는지 결정하는 계층으로 구성된다. 이러한 모델은 언어를 학습하지 않고 알고리즘에 의존하여 사람이 쓴 단어를 이해하고 예를들어, 힙스터 커피 블로그를 제공함으로써 커피에 대한 표준 글을 작성하도록 학습 시킨다. 이 트랜스포머 모델이 대규모 언어 모델 LLM 언어 생성의 기초.  \\n프롬프트는 개발자가 정보를 분석하고 토큰화하기 위해 대규모 언어 모델 LLM에 제공하는 정보로 프롬프트는 기본적으로 다양한 사용 사례에서 LLM에 도움이 되는 학습 데이터 입니다. 더 정확한 프롬프트를 받을수록 LLM은 다음 단어를 더 잘 예측하고 정확한 문장을 구성할 수 있습니다. 따라서 딥러닝 AI의 적절한 학습을 위해서는 적절한 프롬프트를 선택하는 것이 중요하다.\", metadata={'header 1': '대형 언어 모델'}),\n",
       " Document(page_content='↑ This is the date that documentation describing the model\\'s architecture was first released. ↑ In many cases, researchers release or report on multiple versions of a model having different sizes. In these cases, the size of the largest model is listed here. ↑ This is the license of the pre-trained model weights. In almost all cases the training code itself is open-source or can be easily replicated. ↑ The smaller models including 66B are publicly available, while the 175B model is available on request. ↑ Facebook\\'s license and distribution scheme restricted access to approved researchers, but the model weights were leaked and became widely available. ↑ As stated in Technical report: \"Given both the competitive landscape and the safety implications of large-scale models like GPT-4, this report contains no further details about the architecture (including model size), hardware, training compute, dataset construction, training method ...\"[37] Approximate number in the comparison chart that compares the relative storage, from the same report.', metadata={'header 1': '대형 언어 모델', 'header 2': '대형 언어 모델 목록[편집]'}),\n",
       " Document(page_content=\"↑ Goled, Shraddha (2021년 5월 7일). “Self-Supervised Learning Vs Semi-Supervised Learning: How They Differ”. 《Analytics India Magazine》.\\xa0 ↑ 가 나 Devlin, Jacob; Chang, Ming-Wei; Lee, Kenton; Toutanova, Kristina (2018년 10월 11일). “BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding”. arXiv:1810.04805v2 [cs.CL].\\xa0 더 이상 지원되지 않는 변수를 사용함 (도움말) ↑ “BERT”. 2023년 3월 13일 – GitHub 경유.\\xa0 ↑ “BERT, RoBERTa, DistilBERT, XLNet: Which one to use?”.\\xa0[깨진 링크(과거 내용 찾기)] ↑ “GPT-2: 1.5B Release”. 《OpenAI》 (영어). 2019년 11월 5일. 2019년 11월 14일에 원본 문서에서 보존된 문서. 2019년 11월 14일에 확인함.\\xa0 ↑ “Better language models and their implications”. 《openai.com》.\\xa0 ↑ 가 나 “OpenAI's GPT-3 Language Model: A Technical Overview”. 《lambdalabs.com》 (영어).\\xa0 ↑ “gpt-2”. 《GitHub》. 2023년 3월 13일에 확인함.\\xa0 ↑ Wiggers, Kyle (2022년 4월 28일). “The emerging types of language models and why they matter”. 《TechCrunch》.\\xa0 ↑ “GPT Neo”. 2023년 3월 15일 – GitHub 경유.\\xa0 ↑ 가 나 다 Gao, Leo; Biderman, Stella; Black, Sid; Golding, Laurence; Hoppe, Travis; Foster, Charles; Phang, Jason; He, Horace; Thite, Anish; Nabeshima, Noa; Presser, Shawn; Leahy, Connor (2020년 12월 31일). “The Pile: An 800GB Dataset of Diverse Text for Language Modeling”. arXiv:2101.00027 [cs.CL].\\xa0 더 이상 지원되지 않는 변수를 사용함 (도움말) ↑ Iyer, Abhishek (2021년 5월 15일). “GPT-3's free alternative GPT-Neo is something to be excited about”. 《VentureBeat》.\\xa0 ↑ “GPT-J-6B: An Introduction to the Largest Open Source GPT Model | Forefront”. 《www.forefront.ai》 (영어). 2023년 3월 9일에 원본 문서에서 보존된 문서. 2023년 2월 28일에 확인함.\\xa0 ↑ Alvi, Ali; Kharya, Paresh (2021년 10월 11일). “Using DeepSpeed and Megatron to Train Megatron-Turing NLG 530B, the World's Largest and Most Powerful Generative Language Model”. 《Microsoft Research》.\\xa0 ↑ 가 나 Smith, Shaden; Patwary, Mostofa; Norick, Brandon; LeGresley, Patrick; Rajbhandari, Samyam; Casper, Jared; Liu, Zhun; Prabhumoye, Shrimai; Zerveas, George; Korthikanti, Vijay; Zhang, Elton; Child, Rewon; Aminabadi, Reza Yazdani; Bernauer, Julie; Song, Xia (2022년 2월 4일). “Using DeepSpeed and Megatron to Train Megatron-Turing NLG 530B, A Large-Scale Generative Language Model”. arXiv:2201.11990.\\xa0 ↑ Wang, Shuohuan; Sun, Yu; Xiang, Yang; Wu, Zhihua; Ding, Siyu; Gong, Weibao; Feng, Shikun; Shang, Junyuan; Zhao, Yanbin; Pang, Chao; Liu, Jiaxiang; Chen, Xuyi; Lu, Yuxiang; Liu, Weixin; Wang, Xi; Bai, Yangfan; Chen, Qiuliang; Zhao, Li; Li, Shiyong; Sun, Peng; Yu, Dianhai; Ma, Yanjun; Tian, Hao; Wu, Hua; Wu, Tian; Zeng, Wei; Li, Ge; Gao, Wen; Wang, Haifeng (2021년 12월 23일). “ERNIE 3.0 Titan: Exploring Larger-scale Knowledge Enhanced Pre-training for Language Understanding and Generation”. arXiv:2112.12731.\\xa0 ↑ “Product”. 《Anthropic》 (영어). 2023년 3월 14일에 확인함.\\xa0 ↑ 가 나 Askell, Amanda; Bai, Yuntao; Chen, Anna; 외. (2021년 12월 9일). “A General Language Assistant as a Laboratory for Alignment”. arXiv:2112.00861 [cs.CL].\\xa0 더 이상 지원되지 않는 변수를 사용함 (도움말) ↑ 가 나 Dai, Andrew M; Du, Nan (2021년 12월 9일). “More Efficient In-Context Learning with GLaM”. 《ai.googleblog.com》 (영어). 2023년 3월 9일에 확인함.\\xa0 ↑ “Language modelling at scale: Gopher, ethical considerations, and retrieval”. 《www.deepmind.com》 (영어). 2023년 3월 20일에 확인함.\\xa0 ↑ 가 나 다 Hoffmann, Jordan; Borgeaud, Sebastian; Mensch, Arthur; 외. (2022년 3월 29일). “Training Compute-Optimal Large Language Models”. arXiv:2203.15556 [cs.CL].\\xa0 더 이상 지원되지 않는 변수를 사용함 (도움말) ↑ 가 나 Cheng, Heng-Tze; Thoppilan, Romal (2022년 1월 21일). “LaMDA: Towards Safe, Grounded, and High-Quality Dialog Models for Everything”. 《ai.googleblog.com》 (영어). 2023년 3월 9일에 확인함.\\xa0 ↑ Black, Sidney; Biderman, Stella; Hallahan, Eric; 외. (2022년 5월 1일). 《GPT-NeoX-20B: An Open-Source Autoregressive Language Model》. Proceedings of BigScience Episode #5 -- Workshop on Challenges & Perspectives in Creating Large Language Models. 95–136쪽. 2022년 12월 19일에 확인함.\\xa0 ↑ 가 나 다 Hoffmann, Jordan; Borgeaud, Sebastian; Mensch, Arthur; Sifre, Laurent (2022년 4월 12일). “An empirical analysis of compute-optimal large language model training”. 《Deepmind Blog》.\\xa0 ↑ Narang, Sharan; Chowdhery, Aakanksha (2022년 4월 4일). “Pathways Language Model (PaLM): Scaling to 540 Billion Parameters for Breakthrough Performance”. 《ai.googleblog.com》 (영어). 2023년 3월 9일에 확인함.\\xa0 ↑ “Democratizing access to large-scale language models with OPT-175B”. 《ai.facebook.com》 (영어).\\xa0 ↑ Zhang, Susan; Roller, Stephen; Goyal, Naman; Artetxe, Mikel; Chen, Moya; Chen, Shuohui; Dewan, Christopher; Diab, Mona; Li, Xian; Lin, Xi Victoria; Mihaylov, Todor; Ott, Myle; Shleifer, Sam; Shuster, Kurt; Simig, Daniel; Koura, Punit Singh; Sridhar, Anjali; Wang, Tianlu; Zettlemoyer, Luke (2022년 6월 21일). “OPT: Open Pre-trained Transformer Language Models”. arXiv:2205.01068 [cs.CL].\\xa0 더 이상 지원되지 않는 변수를 사용함 (도움말) ↑ 가 나 Khrushchev, Mikhail; Vasilev, Ruslan; Petrov, Alexey; Zinov, Nikolay (2022년 6월 22일), 《YaLM 100B》, 2023년 3월 18일에 확인함\\xa0 ↑ 가 나 Lewkowycz, Aitor; Andreassen, Anders; Dohan, David; Dyer, Ethan; Michalewski, Henryk; Ramasesh, Vinay; Slone, Ambrose; Anil, Cem; Schlag, Imanol; Gutman-Solo, Theo; Wu, Yuhuai; Neyshabur, Behnam; Gur-Ari, Guy; Misra, Vedant (2022년 6월 30일). “Solving Quantitative Reasoning Problems with Language Models”. arXiv:2206.14858 [cs.CL].\\xa0 더 이상 지원되지 않는 변수를 사용함 (도움말) ↑ Ananthaswamy, Anil (2023년 3월 8일). “In AI, is bigger always better?”. 《Nature》.\\xa0 ↑ “bigscience/bloom · Hugging Face”. 《huggingface.co》.\\xa0 ↑ Taylor, Ross; Kardas, Marcin; Cucurull, Guillem; Scialom, Thomas; Hartshorn, Anthony; Saravia, Elvis; Poulton, Andrew; Kerkez, Viktor; Stojnic, Robert (2022년 11월 16일). “Galactica: A Large Language Model for Science”. arXiv:2211.09085 [cs.CL].\\xa0 더 이상 지원되지 않는 변수를 사용함 (도움말) ↑ “20B-parameter Alexa model sets new marks in few-shot learning”. 《Amazon Science》 (영어). 2022년 8월 2일.\\xa0 ↑ Soltan, Saleh; Ananthakrishnan, Shankar; FitzGerald, Jack; 외. (2022년 8월 3일). “AlexaTM 20B: Few-Shot Learning Using a Large-Scale Multilingual Seq2Seq Model”. arXiv:2208.01448 [cs.CL].\\xa0 더 이상 지원되지 않는 변수를 사용함 (도움말) ↑ “AlexaTM 20B is now available in Amazon SageMaker JumpStart | AWS Machine Learning Blog”. 《aws.amazon.com》. 2022년 11월 17일. 2023년 3월 13일에 확인함.\\xa0 ↑ 가 나 “Introducing LLaMA: A foundational, 65-billion-parameter large language model”. 《Meta AI》. 2023년 2월 24일.\\xa0 ↑ “GPT-4 Technical Report” (PDF). 《OpenAI》. 2023. 2023년 3월 14일에 원본 문서 (PDF)에서 보존된 문서. 2023년 3월 14일에 확인함.\\xa0 ↑ Dey, Nolan (2023년 3월 28일). “Cerebras-GPT: A Family of Open, Compute-efficient, Large Language Models”. 《Cerebras》.\\xa0 ↑ 가 나 “Abu Dhabi-based TII launches its own version of ChatGPT”. 《tii.ae》.\\xa0 ↑ Wu, Shijie; Irsoy, Ozan; Lu, Steven; Dabravolski, Vadim; Dredze, Mark; Gehrmann, Sebastian; Kambadur, Prabhanjan; Rosenberg, David; Mann, Gideon (2023년 3월 30일). “BloombergGPT: A Large Language Model for Finance”. arXiv:2303.17564.\\xa0 ↑ Ren, Xiaozhe; Zhou, Pingyi; Meng, Xinfan; Huang, Xinjing; Wang, Yadao; Wang, Weichao; Li, Pengfei; Zhang, Xiaoda; Podolskiy, Alexander; Arshinov, Grigory; Bout, Andrey; Piontkovskaya, Irina; Wei, Jiansheng; Jiang, Xin; Su, Teng; Liu, Qun; Yao, Jun (2023년 3월 19일). “PanGu-Σ: Towards Trillion Parameter Language Model with Sparse Heterogeneous Computing”. arXiv:2303.10845.\\xa0 ↑ Köpf, Andreas; Kilcher, Yannic; von Rütte, Dimitri; Anagnostidis, Sotiris; Tam, Zhi-Rui; Stevens, Keith; Barhoum, Abdullah; Duc, Nguyen Minh; Stanley, Oliver; Nagyfi, Richárd; ES, Shahul; Suri, Sameer; Glushkov, David; Dantuluri, Arnav; Maguire, Andrew (2023년 4월 14일). “OpenAssistant Conversations -- Democratizing Large Language Model Alignment”. 《arXiv:2304.07327 [cs]》.\\xa0 ↑ 가 나 Elias, Jennifer (2023년 5월 16일). “Google's newest A.I. model uses nearly five times more text data for training than its predecessor”. 《CNBC》. 2023년 5월 18일에 확인함.\\xa0  \\n일반텍스트 분석Text segmentation자동 요약기계 번역분포 의미론 모델언어 자원,데이터셋, 말뭉치유형, 표준데이터Automatic identificationand data capture토픽 모델Computer-assistedreviewing자연어 사용자 인터페이스기타 소프트웨어  \\nvte  \\n자연어 처리  \\nAI-완전 단어 가방 모형 n-gram 전산언어학 자연어 이해 불용어 텍스트 처리  \\nBigram Trigram  \\n연어 추출 Concept mining 공통참조해결 Deep linguistic processing Distant reading 정보 추출 개체명 인식 온톨로지 학습 구문 분석 품사 태깅 의미역 결정 의미 유사도 감성 분석 용어 추출 텍스트 마이닝 Textual entailment Truecasing 단어 중의성 해소 Word-sense induction  \\nCompound-term processing 표제어 추출 낱말 분석 Text chunking 어간 추출 문장 분할 단어 분절  \\n다중 문서 요약 문장 추출 텍스트 단순화  \\n컴퓨터 보조 예시 기반 번역 규칙 기반 번역 통계적 전이학습 기반 번역 신경망  \\nBERT 문서-단어 행렬 명시 의미 분석 fastText GloVe 잠재 의미 분석 단어 임베딩 Word2vec  \\n말뭉치언어학 Lexical resource Linguistic Linked Open Data 기계 가독형 사전 병렬말뭉치 PropBank 시맨틱 네트워크 Simple Knowledge Organization System 음성 코퍼스 말뭉치 Thesaurus (information retrieval) Treebank 보편 의존  \\nBabelNet Bank of English 디비피디아 FrameNet 구글 엔그램 뷰어 UBY 워드넷  \\n음성 인식 음성 분할 음성 합성 자연어 생성 광학 문자 인식  \\n문서 분류 잠재 디리클레 할당 파친코 할당  \\n작문 자동 채점 Concordancer 문법 검사기 예측 문자 맞춤법 검사기 문법 예측  \\n챗봇 인터랙티브 픽션 질의 응답 가상 비서 음성 사용자 인터페이스  \\nNatural Language Toolkit spaCy\", metadata={'header 1': '대형 언어 모델', 'header 2': '각주[편집]'}),\n",
       " Document(page_content='원본 주소 \"https://ko.wikipedia.org/w/index.php?title=대형_언어_모델&oldid=35987613\"  \\n분류:  \\n딥 러닝자연어 처리  \\n숨은 분류:  \\n인용 오류 - 오래된 변수를 사용함CS1 - 영어 인용 (en)', metadata={'header 1': '대형 언어 모델'}),\n",
       " Document(page_content='이 문서는 2023년 11월 28일 (화) 22:37에 마지막으로 편집되었습니다. 모든 문서는 크리에이티브 커먼즈 저작자표시-동일조건변경허락 4.0에 따라 사용할 수 있으며, 추가적인 조건이 적용될 수 있습니다. 자세한 내용은 이용 약관을 참고하십시오.Wikipedia®는 미국 및 다른 국가에 등록되어 있는 Wikimedia Foundation, Inc. 소유의 등록 상표입니다.  \\n개인정보처리방침 위키백과 소개 면책 조항 행동 강령 개발자 통계 쿠키 정책 모바일 보기  \\n내용 폭 제한 전환')]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "html_header_splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# chunk 단위로 분리 \n",
    "chunk_size = 500\n",
    "chunk_overlap = 30\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size = chunk_size,\n",
    "    chunk_overlap = chunk_overlap\n",
    ")\n",
    "\n",
    "splits = text_splitter.split_documents(html_header_splits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "37\n",
      "page_content='주 메뉴  \\n사이드바로 이동 숨기기  \\n주 메뉴  \\n둘러보기  \\n대문최근 바뀜요즘 화제임의의 문서로기부  \\n사용자 모임  \\n사랑방사용자 모임관리 요청  \\n편집 안내  \\n소개도움말정책과 지침질문방  \\n검색  \\n검색  \\n계정 만들기 로그인  \\n개인 도구  \\n계정 만들기 로그인  \\n로그아웃한 편집자를 위한 문서 더 알아보기  \\n기여토론  \\n목차 사이드바로 이동 숨기기  \\n처음 위치  \\n1대형 언어 모델 목록  \\n2각주  \\n대형 언어 모델'\n",
      "243\n"
     ]
    }
   ],
   "source": [
    "print(len(splits))\n",
    "print(splits[0])\n",
    "print(len(splits[0].page_content))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CodeTextSplitter\n",
    "# code에 사용되는 syntax 별로 분리 \n",
    "\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter, Language"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['cpp',\n",
       " 'go',\n",
       " 'java',\n",
       " 'kotlin',\n",
       " 'js',\n",
       " 'ts',\n",
       " 'php',\n",
       " 'proto',\n",
       " 'python',\n",
       " 'rst',\n",
       " 'ruby',\n",
       " 'rust',\n",
       " 'scala',\n",
       " 'swift',\n",
       " 'markdown',\n",
       " 'latex',\n",
       " 'html',\n",
       " 'sol',\n",
       " 'csharp',\n",
       " 'cobol',\n",
       " 'c',\n",
       " 'lua',\n",
       " 'perl']"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 사용 가능한 language 목록\n",
    "[e.value for e in Language]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['\\nfunction ',\n",
       " '\\nconst ',\n",
       " '\\nlet ',\n",
       " '\\nvar ',\n",
       " '\\nclass ',\n",
       " '\\nif ',\n",
       " '\\nfor ',\n",
       " '\\nwhile ',\n",
       " '\\nswitch ',\n",
       " '\\ncase ',\n",
       " '\\ndefault ',\n",
       " '\\n\\n',\n",
       " '\\n',\n",
       " ' ',\n",
       " '']"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# js 기준 separators \n",
    "RecursiveCharacterTextSplitter.get_separators_for_language(Language.JS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "JS_CODE = \"\"\"\n",
    "function helloWorld() {\n",
    "  console.log(\"Hello, World!\");\n",
    "}\n",
    "\n",
    "// Call the function\n",
    "helloWorld();\n",
    "\"\"\"\n",
    "\n",
    "js_splitter = RecursiveCharacterTextSplitter.from_language(\n",
    "    language=Language.JS, chunk_size=60, chunk_overlap=0\n",
    ")\n",
    "js_docs = js_splitter.create_documents([JS_CODE])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "page_content='function helloWorld() {\\n  console.log(\"Hello, World!\");\\n}'\n",
      "57\n"
     ]
    }
   ],
   "source": [
    "print(len(js_docs))\n",
    "print(js_docs[0])\n",
    "print(len(js_docs[0].page_content))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split by tokens\n",
    "# 토큰 단위로 split \n",
    "# tiktoken : BPE 알고리즘 기반 tokenizer (by OpenAI)\n",
    "\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(\n",
    "    chunk_size = 100, \n",
    "    chunk_overlap = 0,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = text_splitter.split_text(llm_example_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "27\n",
      "대형 언어 모델(Large language model, LLM) 또는 거대 언어 모델은 수많은 파라미터(보통 수십억 웨이트\n",
      "68\n"
     ]
    }
   ],
   "source": [
    "print(len(texts))\n",
    "print(texts[0])\n",
    "print(len(texts[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "98\n"
     ]
    }
   ],
   "source": [
    "# 토큰 확인\n",
    "import tiktoken\n",
    "\n",
    "encoding = tiktoken.get_encoding(\"gpt2\")\n",
    "encode = encoding.encode(texts[0])\n",
    "\n",
    "print(len(encode))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
