{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ref : \n",
    "\n",
    "https://colab.research.google.com/drive/1YrmsVYrzcvPaWht-yXPbqthiJvHf78qB?usp=sharing%EF%BB%BF\n",
    "\n",
    "https://python.langchain.com/docs/use_cases/chatbots/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OpenAI API Key \n",
    "# 로컬 모델은 일단 너무 느린 듯....\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "load_dotenv('../.env')\n",
    "\n",
    "OPENAI_API_KEY = os.getenv('OPENAI_API_KEY')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.schema import AIMessage, HumanMessage, SystemMessage\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "\n",
    "# default model : gpt-3.5-turbo-0125\n",
    "chat = ChatOpenAI(openai_api_key = OPENAI_API_KEY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='I am a language model AI developed by OpenAI. How can I assist you today?', response_metadata={'token_usage': {'completion_tokens': 18, 'prompt_tokens': 11, 'total_tokens': 29}, 'model_name': 'gpt-3.5-turbo', 'system_fingerprint': 'fp_3b956da36b', 'finish_reason': 'stop', 'logprobs': None}, id='run-d6ac636a-64fb-4719-aa8c-6d1d4effc9dd-0')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# chat test \n",
    "chat(\n",
    "    [\n",
    "        HumanMessage(\n",
    "            content=\"Who are you?\"\n",
    "        )\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'저는 당신을 사랑합니다.'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# conversation chain : 사용자 입력과 모델 출력값을 history로 갖는 메모리 체인 모듈 \n",
    "from langchain.chains import ConversationChain\n",
    "\n",
    "conversation = ConversationChain(llm = chat)\n",
    "conversation.run(\"다음 문장을 한국어로 번역하시오. I love you.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'愛しています。'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversation.run(\"일본어로 번역하시오.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'history': 'Human: hello.\\nAI: hello, sir.'}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# conversation buffer memory : 모든 대화 내용을 기록하는 모듈 \n",
    "from langchain.memory import ConversationBufferMemory\n",
    "\n",
    "memory = ConversationBufferMemory()\n",
    "memory.chat_memory.add_user_message(\"hello.\")\n",
    "memory.chat_memory.add_ai_message(\"hello, sir.\")\n",
    "\n",
    "memory.load_memory_variables({})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'history': \"Human: how are you?\\nAI: I'm fine.\\nHuman: how are you, too?\\nAI: I'm fine, too.\"}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# conversation buffer window memory : 가장 최근 k개 대화 내역 저장 \n",
    "from langchain.memory import ConversationBufferWindowMemory\n",
    "\n",
    "memory = ConversationBufferWindowMemory(k=2)\n",
    "memory.save_context({\"input\": \"hello\"}, {\"output\": \"hello, sir.\"})\n",
    "memory.save_context({\"input\": \"how are you?\"}, {\"output\": \"I'm fine.\"})\n",
    "memory.save_context({\"input\": \"how are you, too?\"}, {\"output\": \"I'm fine, too.\"})\n",
    "\n",
    "memory.load_memory_variables({})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Conversation : 대화형 chatbot 만들기 \n",
    "\n",
    "Memory + prompt + llm 의 구조"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import ChatPromptTemplate, MessagesPlaceholder, SystemMessagePromptTemplate, HumanMessagePromptTemplate\n",
    "from langchain.chains import LLMChain\n",
    "\n",
    "# llm 설정 \n",
    "# open ai 기본 모델 : gpt-3.5-turbo\n",
    "llm = ChatOpenAI(openai_api_key = OPENAI_API_KEY)\n",
    "\n",
    "memory_key = \"chat_history\"\n",
    "\n",
    "# prompt 지정\n",
    "prompt = ChatPromptTemplate(\n",
    "    messages=[\n",
    "        SystemMessagePromptTemplate.from_template(\n",
    "            \"You are chatbot for human.\"\n",
    "        ),\n",
    "        MessagesPlaceholder(variable_name=memory_key), # variable_name : memory 연결 위한 key \n",
    "        HumanMessagePromptTemplate.from_template(\"{question}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "memory = ConversationBufferMemory(memory_key=memory_key, return_messages=True)\n",
    "conversation = LLMChain(llm = llm, prompt = prompt, verbose = True, memory = memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mSystem: You are chatbot for human.\n",
      "Human: hello\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'question': 'hello',\n",
       " 'chat_history': [HumanMessage(content='hello'),\n",
       "  AIMessage(content='Hello! How can I assist you today?')],\n",
       " 'text': 'Hello! How can I assist you today?'}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversation({\"question\" : \"hello\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# memory와 retriver 결합한 chatbot\n",
    "\n",
    "# pdf 파일 가져오기\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "\n",
    "# url을 통해서 pdf 정보 가져오기\n",
    "loader = PyPDFLoader(\"https://snuac.snu.ac.kr/2015_snuac/wp-content/uploads/2015/07/asiabrief_3-26.pdf\")\n",
    "pages = loader.load_and_split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PDF 내용을 작은 chunk 단위로 나누기\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=0)\n",
    "splits = text_splitter.split_documents(pages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vector store에 저장\n",
    "\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "\n",
    "# text-embedding-ada-002\n",
    "vectorstore = Chroma.from_documents(documents=splits, embedding=OpenAIEmbeddings(openai_api_key=OPENAI_API_KEY))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# conversation retriver chain 이용해서 memory + retriever + llm 결합\n",
    "\n",
    "from langchain.chains import ConversationalRetrievalChain\n",
    "\n",
    "retriever = vectorstore.as_retriever()\n",
    "qa = ConversationalRetrievalChain.from_llm(llm, retriever = retriever, memory = memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'question': '저출산을 극복한 나라들은 어디야?',\n",
       " 'chat_history': [HumanMessage(content='hello'),\n",
       "  AIMessage(content='Hello! How can I assist you today?'),\n",
       "  HumanMessage(content='저출산을 극복한 나라들은 어디야?'),\n",
       "  AIMessage(content='중 프랑스, 스웨덴, 영국은 저출산 현상을 극복한 국가들로 알려져 있습니다.')],\n",
       " 'answer': '중 프랑스, 스웨덴, 영국은 저출산 현상을 극복한 국가들로 알려져 있습니다.'}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qa(\"저출산을 극복한 나라들은 어디야?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'question': '스웨덴은 어떻게 저출산 현상을 극복했어?',\n",
       " 'chat_history': [HumanMessage(content='hello'),\n",
       "  AIMessage(content='Hello! How can I assist you today?'),\n",
       "  HumanMessage(content='저출산을 극복한 나라들은 어디야?'),\n",
       "  AIMessage(content='중 프랑스, 스웨덴, 영국은 저출산 현상을 극복한 국가들로 알려져 있습니다.'),\n",
       "  HumanMessage(content='스웨덴은 어떻게 저출산 현상을 극복했어?'),\n",
       "  AIMessage(content='스웨덴은 저출산 현상을 극복하기 위해 여성 고용률을 높이는 등의 다양한 정책을 시행했습니다. 이를 위해 노동시장에서의 젠더 차별을 없애고, 일가정양립 환경을 조성하는 등의 노력을 통해 여성이 일과 가정 생활을 양립할 수 있는 환경을 조성했습니다. 또한, 고령화가 진행됨에 따라 고령인구를 노동시장에 더 오랫동안 남도록 하는 정년 연장 등의 정책도 시행했습니다. 이러한 노력들이 저출산 대책에 기여했을 것으로 보입니다.')],\n",
       " 'answer': '스웨덴은 저출산 현상을 극복하기 위해 여성 고용률을 높이는 등의 다양한 정책을 시행했습니다. 이를 위해 노동시장에서의 젠더 차별을 없애고, 일가정양립 환경을 조성하는 등의 노력을 통해 여성이 일과 가정 생활을 양립할 수 있는 환경을 조성했습니다. 또한, 고령화가 진행됨에 따라 고령인구를 노동시장에 더 오랫동안 남도록 하는 정년 연장 등의 정책도 시행했습니다. 이러한 노력들이 저출산 대책에 기여했을 것으로 보입니다.'}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qa(\"스웨덴은 어떻게 저출산 현상을 극복했어?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
